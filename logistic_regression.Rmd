---
title: "logistic_regression"
author: "Rishika Cherivirala"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(ggplot2)
library(nnet)
library(pROC)

```

```{r}
wine_df <- read.csv("data/wine-quality-white-and-red (1).csv")
head(wine_df)
```

```{r}

quality_counts <- table(wine_df$quality)
print(quality_counts)

```


# Multinomial Logistic Regression
```{r}

wine_df$quality <- as.factor(wine_df$quality)

set.seed(1)

index <- sample(1:nrow(wine_df), 0.7 * nrow(wine_df))
train <- wine_df[index, ]
test <- wine_df[-index, ]

multi_model <- multinom(quality ~ ., data = train)

predictions <- predict(multi_model, test)

confusionMatrix(predictions, test$quality)


```
## ROC Curve

```{r}

predicted_probs_multi <- predict(multi_model, test, type = "probs")

true_labels <- test$quality

roc_list <- lapply(levels(true_labels), function(class_label) {
  binary_labels <- ifelse(true_labels == class_label, 1, 0)
  
  roc(binary_labels, predicted_probs_multi[, class_label])
})

plot(roc_list[[1]], col = "red", main = "Multinomial ROC Curves", lwd = 2)
for (i in 2:length(roc_list)) {
  lines(roc_list[[i]], col = i, lwd = 2)
}

legend("bottomright", legend = levels(true_labels), col = 1:length(roc_list), lwd = 2)

sapply(roc_list, auc)


```

##Cross Validation

```{r}

# wine_df$quality <- as.factor(wine_df$quality)
# 
# control <- trainControl(method = "cv", number = 10)
# 
# set.seed(1)
# multi_model_cv <- train(quality ~ ., data = wine_df, method = "multinom", trControl = control)
# 
# print(multi_model_cv)
# 
# predictions <- predict(multi_model_cv, newdata = test)
# 
# confusionMatrix(predictions, test$quality)



```
Accuracy= 0.5477

## RMSE


#Logistic Regression

Since logistic regression only has 2 output variables: 0 or 1 or "Yes" or "No". I decided to split the data in a way where wines that have a quality >= 7, are 0 and wines that have a quality < 7, are 1. 

(https://vineroutes.com/wine-rating-system) shows the scale for wine quality.
```{r}

wine_df <- wine_df %>%
  mutate(quality = as.numeric(as.character(quality))) %>%
  mutate(quality_binary = ifelse(quality >= 7, 1, 0)) %>%
  mutate(quality_binary = as.factor(quality_binary))

set.seed(1)
index2 <- sample(1:nrow(wine_df), 0.7 * nrow(wine_df))
train2 <- wine_df[index2, ]
test2 <- wine_df[-index2, ]

log_model <- glm(quality_binary ~ . - quality, data = train2, family = binomial)
summary(log_model)

predictions2 <- predict(log_model, test2, type = "response")
predicted_classes2 <- ifelse(predictions2 > 0.5, "1", "0")

confusionMatrix(as.factor(predicted_classes2), test2$quality_binary)

```
```{r}

log2_index <- sample(1:nrow(wine_df), 0.7 * nrow(wine_df))
log2_train <- wine_df[log2_index, ]
log2_test <- wine_df[-log2_index, ]

log_model2 <- glm(quality_binary ~ . - quality - type - citric.acid, data = log2_train, family = binomial)

log_predictions2 <- predict(log_model2, log2_test, type = "response")

log_predicted_classes2 <- ifelse(log_predictions2 > 0.5, "1", "0")



```


## ROC Curve
```{r}
predictions_prob <- predict(log_model, test2, type = "response")

roc_curve <- roc(test2$quality_binary, predictions_prob)
plot(roc_curve, col = "blue", main = "ROC Curve")

auc(roc_curve)

```
AUC = 0.8028. That means the model does an okay job in predicting it and is not completely due to random chance.


## Cross Validation

```{r}

control2 <- trainControl(method = "cv", number = 10)

set.seed(1)
cv_model <- train(quality_binary ~ . - quality, data = wine_df, method = "glm", family = binomial, trControl = control2)

print(cv_model)

predictions <- predict(cv_model, newdata = test2)

confusionMatrix(predictions, test2$quality_binary)

```
Since the data is a little skewed with a lot more samples landing in the 0 category than the 1 category, which might affect the accuracy of the model, I'm going to try adding weights to the regression to try to make it more accurate

## Weighted Logistic Regression
```{r}

wine_df <- wine_df %>%
  mutate(quality = as.numeric(as.character(quality))) %>%
  mutate(quality_binary = ifelse(quality >= 7, 1, 0)) 

set.seed(1)
index3 <- sample(1:nrow(wine_df), 0.7 * nrow(wine_df))
train3 <- wine_df[index3, ]
test3 <- wine_df[-index3, ]

weights <- ifelse(train3$quality_binary == 1, 0.8, 0.2) 

log_model_weighted <- glm(quality_binary ~ . - quality, data = train3, family = binomial, weights = weights)

predictions_weighted <- predict(log_model_weighted, test3, type = "response")
predicted_classes_weighted <- ifelse(predictions_weighted > 0.5, "1", "0")

confusionMatrix(as.factor(predicted_classes_weighted), as.factor(test3$quality_binary))

predictions_prob2 <- predict(log_model_weighted, test3, type = "response")


```

```{r}
# For normal logistic regression
roc_normal <- roc(test2$quality_binary, predictions_prob)
plot(roc_normal, col = "red", main = "ROC Curve Comparison")
auc(roc_normal)

# For weighted logistic regression
roc_weighted <- roc(test3$quality_binary, predictions_prob2)
plot(roc_weighted, col = "blue", add = TRUE)
auc(roc_weighted)
```
The ROC curves for weighted/normal are basically the same. So the model's discriminatory power did not really improve.

## Balances Logistic Regression

```{r}
minority_class <- wine_df %>% filter(quality_binary == 1)
majority_class <- wine_df %>% filter(quality_binary == 0)

set.seed(1)
majority_class_undersampled <- majority_class %>%
  sample_n(nrow(minority_class))  

balanced_df <- bind_rows(minority_class, majority_class_undersampled)

set.seed(1)
index_balanced <- sample(1:nrow(balanced_df), 0.7 * nrow(balanced_df))
train_balanced <- balanced_df[index_balanced, ]
test_balanced <- balanced_df[-index_balanced, ]

log_model_balanced <- glm(quality_binary ~ . - quality, data = train_balanced, family = binomial)

predictions_balanced <- predict(log_model_balanced, test_balanced, type = "response")
predicted_classes_balanced <- ifelse(predictions_balanced > 0.5, "1", "0")

confusionMatrix(as.factor(predicted_classes_balanced), as.factor(test_balanced$quality_binary))
```
```{r}
# For normal logistic regression
roc_normal <- roc(test2$quality_binary, predictions_prob)
plot(roc_normal, col = "red", main = "ROC Curve Comparison", lwd = 2)

auc_normal <- auc(roc_normal)
cat("AUC for Normal Logistic Regression:", auc_normal, "\n")

# For weighted logistic regression
roc_weighted <- roc(test3$quality_binary, predictions_prob2)
lines(roc_weighted, col = "blue", lwd = 2)

auc_weighted <- auc(roc_weighted)
cat("AUC for Weighted Logistic Regression:", auc_weighted, "\n")

# For balanced logistic regression
roc_balanced <- roc(test_balanced$quality_binary, predictions_balanced)
lines(roc_balanced, col = "green", lwd = 2)

auc_balanced <- auc(roc_balanced)
cat("AUC for Undersampled Logistic Regression:", auc_balanced, "\n")

# Add Legend to the Plot
legend("bottomright", 
       legend = c("Normal Logistic", "Weighted Logistic", "Undersampled Logistic"), 
       col = c("red", "blue", "green"), 
       lwd = 2)

```
### RMSE

```{r}
actual_normal <- as.numeric(as.character(test2$quality_binary))

rmse_normal <- sqrt(mean((predictions_prob - actual_normal)^2))
cat("RMSE for Normal Logistic Regression:", rmse_normal, "\n")

actual_weighted <- as.numeric(as.character(test3$quality_binary))

rmse_weighted <- sqrt(mean((predictions_prob2 - actual_weighted)^2))
cat("RMSE for Weighted Logistic Regression:", rmse_weighted, "\n")

actual_balanced <- as.numeric(as.character(test_balanced$quality_binary))

rmse_balanced <- sqrt(mean((predictions_balanced - actual_balanced)^2))
cat("RMSE for Undersampled Logistic Regression:", rmse_balanced, "\n")

actual_normal2 <- as.numeric(as.character(log2_test$quality_binary))
rmse_log2 <- sqrt(mean((log_predictions2 - actual_normal2)^2))
cat("RMSE for Specific Predictors Logistic Regression:", rmse_log2, "\n")

AIC(log_model, log_model_weighted, log_model_balanced, log_model2)
```

