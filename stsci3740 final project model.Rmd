---
title: "stsci3740 final project modeling"
author: "Fiona Huang"
date: "2025-04-25"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(leaps)
library(tidyverse)
library(corrplot)
library(class)
library(caret)
library(e1071)
```


```{r}
# import datasets
red <- read.csv("C:/Users/xinya/Downloads/Cornell Classes/STSCI 3740/final project/winequality-red.csv", sep=";")

white <- read.csv("C:/Users/xinya/Downloads/Cornell Classes/STSCI 3740/final project/winequality-white.csv", sep=";")

wine <- read.csv("C:/Users/xinya/Downloads/Cornell Classes/STSCI 3740/final project/wine-quality-white-and-red.csv")
```


## Fitting a KNN Model

```{r}
# normalize the data using z-score
normalize <- function(x) {
  return((x - mean(x)) / sd(x))
}

# wine_norm <- wine %>% 
#   mutate(across(where(is.numeric) & !where(is.factor), normalize))

all_columns <- names(wine)
columns_to_normalize <- all_columns[all_columns != "quality" & sapply(wine, is.numeric)]
wine_norm <- wine
wine_norm[columns_to_normalize] <- lapply(wine[columns_to_normalize], normalize)



# change type of wine to white=1, red=2
wine_norm$type <- as.numeric(factor(wine_norm$type))

# split the dataset into train/test
set.seed(1)
index <- sample(1:nrow(wine_norm), size=nrow(wine_norm)*0.7, rep=FALSE)
training <- wine_norm[index, ]
testing <- wine_norm[-index, ]

training_X <- training %>% select(-quality)
testing_X <- testing %>% select(-quality)
```


```{r}
# try different values of k
k.values <- 1:20

knn.errors <- sapply(k.values, function(k) {
  knn.pred <- knn(training_X, testing_X, training$quality, k=k)
  mean(knn.pred != testing$quality)
})

print(knn.errors)

```

The value of k that seems to perform the best on this data is k=1.


```{r}
set.seed(1)
# 10-fold cross validation 
control <- trainControl(method = "cv", number = 10)

knn_cv <- train(
  quality ~ .,
  data = wine_norm,
  method = "knn",
  trControl = control,
  tuneGrid = expand.grid(k = 1:20)
)

knn_cv
plot(knn_cv)

```


```{r}
model <- knn(training_X, testing_X, training$quality, k=10)

confusion_matrix <- table(Predicted = model, Actual = testing$quality)
print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy
  
```



```{r eval=FALSE}
# fitting the knn models using less number of variables
X <- c("alcohol", "volatile.acidity", "residual.sugar", "total.sulfur.dioxide")

# use RFE (recursive feature elimination to select different subsets)
rfe_ctrl <- rfeControl(functions = caretFuncs, method = "cv", number = 10)
rfe_model <- rfe(wine_norm %>% select(-quality),
                 wine_norm$quality,
                 sizes = c(2:11), # test sets with 2 to 11 variables
                 rfeControl = rfe_ctrl,
                 method = "knn")

# Best variable set
print(rfe_model$optVariables)

```

```{r eval=FALSE}
# plot the model performance as a function of number of variables
plot(rfe_model, type = c("g", "o"))
```



```{r}
new_wine <- wine
new_wine$type <- as.numeric(factor(new_wine$type))

# Fit a linear model
lm_model <- lm(quality ~ ., data = new_wine)

# Stepwise model selection
step_model <- step(lm_model, direction = "backward")
step_model
```

```{r}
# try stepwise with normalized data

# Fit a linear model
lm_model <- lm(quality ~ ., data = wine_norm)

# Stepwise model selection
step_model <- step(lm_model, direction = "both")
step_model
```


```{r}
# best subset selection
library(leaps)

best_fit <- regsubsets(quality ~ ., data = new_wine, nvmax = 11)
summary(best_fit)

# find model size with lowest BIC
summary_best <- summary(best_fit)
which.min(summary_best$bic)

plot(summary_best$bic, type = "b", main = "BIC for Best Subset Models")
points(which.min(summary_best$bic), summary_best$bic[which.min(summary_best$bic)],
       col = "red", pch = 19)
```


```{r}
# apply PCA before fitting KNN
new_wine <- wine
new_wine$type <- as.numeric(factor(new_wine$type))

target <- new_wine$quality
predictors <- new_wine %>% select(-quality)

# standardize data
scaled_data <- scale(predictors)

# Perform PCA
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
summary(pca_result)

# reduce dimensionality
# choose the first two PCs
pca_data <- pca_result$x[, 1:2]
```

```{r}
# plot the first two PCs
pca_df <- data.frame(PC1 = pca_data[, 1], PC2 = pca_data[, 2], Target = target)
ggplot(pca_df, aes(x = PC1, y = PC2, color = Target)) +
  geom_point() +
  theme_minimal() +
  ggtitle("PCA Plot of the Data")
```


```{r}
# apply KNN on PCA-reduced data
set.seed(1)
index_2 <- sample(1:nrow(pca_data), size=nrow(pca_data)*0.7, rep=FALSE)
training_data <- pca_data[index_2, ]
testing_data <- pca_data[-index_2, ]

training_targt <- target[index_2]
testing_target <- target[-index_2]

# 10-fold cross validation 
control_new <- trainControl(method = "cv", number = 10)

knn_cv_2 <- train(
  training_data, 
  training_targt, 
  method = "knn",
  trControl = control_new,
  tuneGrid = expand.grid(k = 1:20)
)

knn_cv_2
plot(knn_cv_2)

knn_model <- knn(train = training_data, test = testing_data, cl = training_targt, k = 17)

confusion_matrix <- table(Predicted = knn_model, Actual = testing_target)
print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy

```

