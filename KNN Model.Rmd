---
title: "KNN Model"
author: "Isabella Chen (ic324), Rishika Cherivirala (rrc87), Fiona Huang (xh393), Katie Perlitz (kap256)"
date: "2025-04-25"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(leaps)
library(tidyverse)
library(corrplot)
library(class)
library(caret)
library(e1071)
```


```{r}
# import datasets
wine <- read.csv("C:/Users/xinya/Downloads/Cornell Classes/STSCI 3740/final project/wine-quality-white-and-red.csv")
```


## Fitting a KNN Model

```{r}
# normalize the data using z-score
normalize <- function(x) {
  return((x - mean(x)) / sd(x))
}

all_columns <- names(wine)
columns_to_normalize <- all_columns[all_columns != "quality" & sapply(wine, is.numeric)]
wine_norm <- wine
wine_norm[columns_to_normalize] <- lapply(wine[columns_to_normalize], normalize)


# change type of wine to white=1, red=2
wine_norm$type <- as.numeric(factor(wine_norm$type))

# split the dataset into train/test
set.seed(1)
index <- sample(1:nrow(wine_norm), size=nrow(wine_norm)*0.7, rep=FALSE)
training <- wine_norm[index, ]
testing <- wine_norm[-index, ]

training_X <- training %>% select(-quality)
testing_X <- testing %>% select(-quality)
```


```{r}
# try different values of k from 1 to 20
k.values <- 1:20

knn.errors <- sapply(k.values, function(k) {
  knn.pred <- knn(training_X, testing_X, training$quality, k=k)
  mean(knn.pred != testing$quality)
})

print(knn.errors)

```

The value of k that seems to perform the best on this data is k=1.


## Choose the optimal k-value using Cross Validation

```{r}
set.seed(1)
# 10-fold cross validation 
control <- trainControl(method = "cv", number = 10)

knn_cv <- train(
  quality ~ .,
  data = wine_norm,
  method = "knn",
  trControl = control,
  tuneGrid = expand.grid(k = 1:20)
)

knn_cv
plot(knn_cv)

```

## Fitting the best model

```{r}
model <- knn(training_X, testing_X, training$quality, k=10)

confusion_matrix <- table(coPredicted = model, Actual = testing$quality)
print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy
  
```

## Apply PCA to reduce diensionality

```{r}
# apply PCA before fitting KNN
new_wine <- wine
new_wine$type <- as.numeric(factor(new_wine$type))

target <- new_wine$quality
predictors <- new_wine %>% select(-quality)

# standardize data
scaled_data <- scale(predictors)

# Perform PCA
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
summary(pca_result)
```

Decided to choose the first eight PCs

```{r}
# reduce dimensionality
# choose the first eight PCs
pca_data <- pca_result$x[, 1:8]
```

## Fit KNN Model on PCA-reduced data

```{r}
# apply KNN on PCA-reduced data
set.seed(1)
index_2 <- sample(1:nrow(pca_data), size=nrow(pca_data)*0.7, rep=FALSE)
training_data <- pca_data[index_2, ]
testing_data <- pca_data[-index_2, ]

training_targt <- target[index_2]
testing_target <- target[-index_2]

# 10-fold cross validation to find the best k
control_new <- trainControl(method = "cv", number = 10)

knn_cv_2 <- train(
  training_data, 
  training_targt, 
  method = "knn",
  trControl = control_new,
  tuneGrid = expand.grid(k = 1:20)
)

knn_cv_2
plot(knn_cv_2)

knn_model <- knn(train = training_data, test = testing_data, cl = training_targt, k = 17)

confusion_matrix <- table(Predicted = knn_model, Actual = testing_target)
print(confusion_matrix)

accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy

```


```{r}

```

